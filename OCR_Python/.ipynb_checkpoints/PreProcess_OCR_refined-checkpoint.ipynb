{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d96ca7c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we use Amazon Textract and Google Vision to provide a quick way of extracting text/tables from an image of a page.\n",
    "\n",
    "Intended use: The intended use of this notebook is to quickly prototype. You should expect to modify the code in this notebook to suit your usecase.\n",
    "\n",
    "Preparation: At a minimum, set a working folder, and make sure to add your API keys for both Textract and Google Vision. To do so, please follow the steps outlined here: https://github.com/MikeJGiordano/OCR_History/blob/main/ReadMe.md\n",
    "\n",
    "This notebook contains four parts:\n",
    "\n",
    "    1. Unmodified image OCR. This is intended to quickly detect text from a single image.\n",
    "        a. There is then an option to run one or both OCR tools on a whole folder.\n",
    "        \n",
    "    2. Image preprocessing. This routine helps you to quickly preprocess a single image (adjust contrast, split image, etc). \n",
    "        a. If you are satisfied with the preprocessing routine, it will give you the option to preprocess a whole folder.\n",
    "        \n",
    "    3. Image preprocessing with text extraction. This runs the image modification from part 2 into the text detection from part 1.\n",
    "    \n",
    "    4. Image preprocessing with table extraction from Textract. This uses the image modification from part 2 to extract a table using Textract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17f3fe1",
   "metadata": {},
   "source": [
    "# Program Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2655be",
   "metadata": {},
   "source": [
    "## There are 5 steps, marked A-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45e665c-4bdb-43d8-98e5-1fe7e4f986ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Google Cloud setup:\n",
      "Credentials file path: /mnt/c/Users/WATLINGS/Documents/GitHub/OCR_History/OCR_Python/ServiceAccountToken.json\n",
      "Credentials file exists: True\n",
      "Successfully created Vision client\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\n",
    "import os\n",
    "\n",
    "# Check credentials\n",
    "print(\"Checking Google Cloud setup:\")\n",
    "if 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ:\n",
    "    cred_path = os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "    print(f\"Credentials file path: {cred_path}\")\n",
    "    print(f\"Credentials file exists: {os.path.exists(cred_path)}\")\n",
    "else:\n",
    "    print(\"GOOGLE_APPLICATION_CREDENTIALS environment variable not set\")\n",
    "\n",
    "# Try to create a client\n",
    "try:\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    print(\"Successfully created Vision client\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Vision client: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c497df2",
   "metadata": {},
   "source": [
    "### A: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af15c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "# if you don't have these packages use any package manager to install\n",
    "# you can install all packages at once using the provided requirements.txt file\n",
    "import cv2\n",
    "import boto3\n",
    "from google.cloud import vision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tq\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures, Direction, DirectionalFinderType\n",
    "\n",
    "# note: the following py file, you'll have to download\n",
    "import preprocess as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7098cf",
   "metadata": {},
   "source": [
    "### B: Please set your working directories here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02996d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please set the path to the folder containing your images here\n",
    "input_folder = \"images\"  # relative path since we're already in the correct directory\n",
    "# please set the path to a desired output folder here\n",
    "output_folder = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56c857",
   "metadata": {},
   "source": [
    "### C: Please set your main input file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa6b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filename to your image here\n",
    "newspaper_image = \"NYT.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a66c0d",
   "metadata": {},
   "source": [
    "### D: Please authenticate Google Cloud\n",
    "\n",
    "For help with Google Cloud, see https://github.com/MikeJGiordano/OCR_History/blob/main/Setup_Google_Cloud.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9adfe73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authenticate Google Cloud here\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/mnt/c/Users/WATLINGS/Documents/GitHub/OCR_History/OCR_Python/ServiceAccountToken.json'\n",
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658deca",
   "metadata": {},
   "source": [
    "### E: Please authenticate Amazon Textract\n",
    "\n",
    "For help with Amazon Textract, see https://github.com/MikeJGiordano/OCR_History/blob/main/Setup_AWS_Root.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6044585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authenticate AWS Textract in the console/terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4334c36",
   "metadata": {},
   "source": [
    "# Part 1: Basic text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc215ab-3027-4669-a76b-2f26316b5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from google.cloud import vision\n",
    "\n",
    "# Convert all Windows paths to WSL paths\n",
    "def to_wsl_path(windows_path):\n",
    "    return windows_path.replace('C:', '/mnt/c').replace('\\\\', '/')\n",
    "\n",
    "# Convert all paths\n",
    "wsl_image_path = to_wsl_path(r\"C:\\Users\\WATLINGS\\Documents\\GitHub\\OCR_History\\OCR_Python\\images\\NYT.png\")\n",
    "wsl_input_folder = to_wsl_path(r\"C:\\Users\\WATLINGS\\Documents\\GitHub\\OCR_History\\OCR_Python\\images\")\n",
    "wsl_output_folder = to_wsl_path(r\"C:\\Users\\WATLINGS\\Documents\\GitHub\\OCR_History\\OCR_Python\\output\")\n",
    "\n",
    "# Verify all paths exist\n",
    "print(\"Checking paths:\")\n",
    "print(f\"Image exists: {os.path.exists(wsl_image_path)}\")\n",
    "print(f\"Input folder exists: {os.path.exists(wsl_input_folder)}\")\n",
    "print(f\"Output folder exists: {os.path.exists(wsl_output_folder)}\")\n",
    "\n",
    "try:\n",
    "    # First test direct Vision API access\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    print(\"Successfully created Vision client\")\n",
    "    \n",
    "    # Test direct image processing\n",
    "    with open(wsl_image_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    \n",
    "    image = vision.Image(content=content)\n",
    "    response = client.text_detection(image=image)\n",
    "    if response.error.message:\n",
    "        print(f\"API Error: {response.error.message}\")\n",
    "    else:\n",
    "        print(\"Direct Vision API call successful!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Detailed error information: {str(e)}\")\n",
    "    import traceback\n",
    "    print(f\"Full traceback: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bb5f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input variables:\n",
      "newspaper_image type: <class 'str'>\n",
      "input_folder path exists: True\n",
      "output_folder path exists: True\n",
      "Starting process_content...\n",
      "Google Vision Output:\n",
      "Error with Cloud Vision\n",
      "Setting all parameters=True gives a basic visualization of the outputs of both Cloud Vision, defaulted as the first image, and Textract, the second image. The .txt and .json outputs for both Cloud Vision and Textract are saved in the output_folder. By setting a parameter=False, you can skip that function. For example, if use_textract=False and use_google_vision=True, this will not send the image through Textract, but will send the image through Google Vision.\n",
      "process_content completed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Add this before your process_content call to check the variables\n",
    "print(f\"Checking input variables:\")\n",
    "print(f\"newspaper_image type: {type(newspaper_image)}\")\n",
    "print(f\"input_folder path exists: {os.path.exists(input_folder)}\")\n",
    "print(f\"output_folder path exists: {os.path.exists(output_folder)}\")\n",
    "\n",
    "# Add this debugging code right before your process_content call\n",
    "try:\n",
    "    print(\"\\nTesting direct Vision API call...\")\n",
    "    from google.cloud import vision\n",
    "    \n",
    "    # Create vision client\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    # Construct the full image path\n",
    "    full_image_path = os.path.join(input_folder, newspaper_image)\n",
    "    print(f\"Attempting to read image from: {full_image_path}\")\n",
    "    \n",
    "    # Read the image file\n",
    "    with open(full_image_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    \n",
    "    # Create vision image object\n",
    "    image = vision.Image(content=content)\n",
    "    \n",
    "    # Try text detection\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    \n",
    "    print(f\"Number of text blocks found: {len(texts)}\")\n",
    "    if len(texts) > 0:\n",
    "        print(\"First text block found:\", texts[0].description[:100])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Direct Vision API test failed with error: {str(e)}\")\n",
    "\n",
    "# Then your original process_content call\n",
    "print(\"\\nNow trying process_content...\")\n",
    "\n",
    "try:\n",
    "    print(\"Starting process_content...\")\n",
    "    pp.process_content(newspaper_image, \n",
    "                      input_folder,\n",
    "                      output_folder,\n",
    "                      show_image=True,\n",
    "                      use_google_vision=True, \n",
    "                      use_textract=False, \n",
    "                      verbose=True)\n",
    "    print(\"process_content completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Detailed error information: {str(e)}\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    import traceback\n",
    "    print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "    \n",
    "    # Check Google Cloud credentials\n",
    "    import os\n",
    "    print(\"\\nChecking Google Cloud credentials:\")\n",
    "    if 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ:\n",
    "        print(f\"Credentials path: {os.environ['GOOGLE_APPLICATION_CREDENTIALS']}\")\n",
    "        print(f\"File exists: {os.path.exists(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])}\")\n",
    "    else:\n",
    "        print(\"GOOGLE_APPLICATION_CREDENTIALS not set in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fbd4d",
   "metadata": {},
   "source": [
    "### You can use the next cell to get text and JSON files for the entire input folder through Google Vision, Textract, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec56256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|█████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 9679.16image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images OCR'd. text and JSON files are in folder output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch process all images in the input folder, save text and JSON outputs to the output folder\n",
    "\n",
    "pp.batch_ocr(input_folder, \n",
    "                 output_folder, \n",
    "                 use_google_vision=False, \n",
    "                 use_textract=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-glossary",
   "metadata": {},
   "source": [
    "# Part 2: Preprocess images\n",
    "Often, it helps to preprocess an image. Common routines are:\n",
    "    \n",
    "    1. Adjusting contrast or brightness\n",
    "    2. Converting to grayscale\n",
    "    3. Cropping\n",
    "    4. Erasing margins\n",
    "    5. Splitting images\n",
    "    \n",
    "We now provide two examples:\n",
    "    \n",
    "    1. Applying points 1-4 \n",
    "    2. Preprocessing and splitting the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9a192",
   "metadata": {},
   "source": [
    "### Example 1: Full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "running-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filename to your image here\n",
    "railroad_table = \"1888_Page_161.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2976260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next cell will apply the default preprocess settings to your image.\n",
    "#If you are unsatisfied with those settings, it will instruct you on how to make changes.\n",
    "#Those changes should be inserted in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba78acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to split this image into two separate images? (y/n): y\n",
      "Do you want to split it Vertically or Horizontally? (v/h) y\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.core.multiarray' has no attribute 'integer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Preprocess a single image.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrailroad_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                       \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/WATLINGS/Documents/GitHub/OCR_History/OCR_Python/preprocess.py:264\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(filename, input_folder, output_folder, left_margin_percent, top_margin_percent, vsplit_percent, hsplit_percent, brightness_factor, contrast_factor)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Create a copy of the image to draw margin lines\u001b[39;00m\n\u001b[1;32m    262\u001b[0m MarginTest \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 264\u001b[0m MarginTest \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMarginTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrightness_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_margin_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_margin_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvsplit_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhsplit_percent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Draw margin lines\u001b[39;00m\n\u001b[1;32m    267\u001b[0m cv2\u001b[38;5;241m.\u001b[39mline(MarginTest, (LeftMargin, TopMargin), (RightMargin, TopMargin), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/WATLINGS/Documents/GitHub/OCR_History/OCR_Python/preprocess.py:722\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image, brightness_factor, contrast_factor, left_margin_percent, top_margin_percent, vsplit_percent, hsplit_percent)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image, brightness_factor, contrast_factor, left_margin_percent, top_margin_percent, vsplit_percent, hsplit_percent):\n\u001b[1;32m    720\u001b[0m     \n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# Convert to grayscale if needed\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mgrayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# Apply brightness adjustment\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m brightness_factor \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/c/Users/WATLINGS/Documents/GitHub/OCR_History/OCR_Python/preprocess.py:664\u001b[0m, in \u001b[0;36mgrayscale\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    661\u001b[0m b, g, r \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msplit(image)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# Compute the per-channel means\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m b_mean \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m g_mean \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    666\u001b[0m r_mean \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/python_work/lib/python3.9/site-packages/numpy/_core/_methods.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m umath \u001b[38;5;28;01mas\u001b[39;00m um\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asanyarray\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numerictypes \u001b[38;5;28;01mas\u001b[39;00m nt\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _exceptions\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ufunc_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _no_nep50_warning\n",
      "File \u001b[0;32m~/miniconda3/envs/python_work/lib/python3.9/site-packages/numpy/_core/numerictypes.py:102\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# we don't need all these imports, but we need to keep them for compatibility\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# for users using np._core.numerictypes.UPPER_TABLE\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_string_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     99\u001b[0m     english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE\n\u001b[1;32m    100\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_type_aliases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    103\u001b[0m     sctypeDict, allTypes, sctypes\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _kind_name\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# we don't export these for import *, but we do want them accessible\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# as numerictypes.bool, etc.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_work/lib/python3.9/site-packages/numpy/_core/_type_aliases.py:38\u001b[0m\n\u001b[1;32m     31\u001b[0m _abstract_type_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minexact\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflexible\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharacter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplexfloating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsignedinteger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignedinteger\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _abstract_type_name \u001b[38;5;129;01min\u001b[39;00m _abstract_type_names:\n\u001b[0;32m---> 38\u001b[0m     allTypes[_abstract_type_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_abstract_type_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m typeinfo\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNPY_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m c_names_dict:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.core.multiarray' has no attribute 'integer'"
     ]
    }
   ],
   "source": [
    "#Preprocess a single image.\n",
    "pp.preprocess_image(railroad_table,\n",
    "                       input_folder,\n",
    "                       output_folder,\n",
    "                       **pp.default);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c663b",
   "metadata": {},
   "source": [
    "### Example 2: Split image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filename to your split image here\n",
    "korean_image = \"126.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next cell will apply the default preprocess settings to your image.\n",
    "#If you are unsatisfied with those settings, it will provide instructions on how to make changes.\n",
    "\n",
    "pp.default['left_margin_percent'] = 30\n",
    "pp.default['top_margin_percent'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26302043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess a split image.\n",
    "pp.preprocess_image(korean_image,\n",
    "                       input_folder,\n",
    "                       output_folder,\n",
    "                       **pp.default);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea7b20",
   "metadata": {},
   "source": [
    "# Part 3: Preprocessed Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba520df",
   "metadata": {},
   "source": [
    "### Example 1: Full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above processing, the folder of modified images is located at:\n",
    "\n",
    "modified_images = \"output/modified_images/\"\n",
    "\n",
    "# Modification alters the name of the file to be:\n",
    "\n",
    "modified_railroad = 'modified_' + railroad_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f58288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the image, save .json outputs\n",
    "pp.process_content(modified_railroad, \n",
    "                   modified_images,\n",
    "                   output_folder,\n",
    "                   show_image = True,\n",
    "                   use_google_vision=False, \n",
    "                   use_textract=True, \n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a4778",
   "metadata": {},
   "source": [
    "### Example 2: Split image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification splits the file into two and renames them:\n",
    "\n",
    "modified_1_split = 'modified_1_' + korean_image\n",
    "modified_2_split = 'modified_2_' + korean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ac7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the images, save .json and .txt outputs\n",
    "pp.process_content(modified_1_split, \n",
    "                   modified_images,\n",
    "                   output_folder,\n",
    "                   show_image = True,\n",
    "                   use_google_vision=True, \n",
    "                   use_textract=False, \n",
    "                   verbose=True)\n",
    "\n",
    "pp.process_content(modified_2_split, \n",
    "                   modified_images,\n",
    "                   output_folder,\n",
    "                   show_image = False,\n",
    "                   use_google_vision=False, \n",
    "                   use_textract=False, \n",
    "                   verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddbd16",
   "metadata": {},
   "source": [
    "### You can use the next cell to get text and JSON files for the entire folder of modified images through Google Vision, Textract, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfff978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process all images in the modified folder, save .json outputs to the output folder\n",
    "\n",
    "pp.batch_ocr(modified_images, \n",
    "                 output_folder, \n",
    "                 use_google_vision=False, \n",
    "                 use_textract=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdc2f2",
   "metadata": {},
   "source": [
    "# Part 4: Textract Table Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e55ea7",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8cb3f4",
   "metadata": {},
   "source": [
    "Initialize Textractor client, modify region if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Textractor(profile_name=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6aa52b",
   "metadata": {},
   "source": [
    "Please specify the image you want to extract a table from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8150da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above processing, the folder of modified images is located at:\n",
    "\n",
    "modified_images = \"output/modified_images/\"\n",
    "\n",
    "# Modification alters the name of the file to be:\n",
    "\n",
    "modified_railroad = 'modified_' + railroad_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce277761",
   "metadata": {},
   "source": [
    "## Extract the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.extract_table(extractor, \n",
    "                       modified_railroad,\n",
    "                       modified_images,\n",
    "                       output_folder);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
